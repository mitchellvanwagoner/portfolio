---
title: "Comedy Robot - Capstone Project"
slug: capstone
---

<section>
## Overview

<ImageTextBlock src="/images/projects/capstone/capstone-46.webp" alt="Comedy Robot performing on table" caption="NAO robot performing comedy on custom-built table" size="lg" position="left">

As robots become more integrated into our everyday lives, there exists the need for robots to be able to engage naturally with humans. Our team developed a robotic system capable of performing a comedy routine while interacting with its audience and adjusting its performance based on real-time reactions.

This project extends Professor Naomi Fitter's previous work on robotic comedians, enhancing the NAO V6 robot's capability to interpret audience reactions through visual and auditory inputs dynamically using facial expression reading and Large Language Models (LLMs).

</ImageTextBlock>

**Team:** Kevin Sabbe, Daniel Eastman, Mitchell Van Wagoner

**Project Sponsor:** Professor Naomi Fitter | **Instructor:** Dr. Jayani Jayasuriya

<VideoEmbed src="https://www.youtube.com/embed/JUW971972gY" title="Showcase Video" />

<CarouselGallery
  title="Project Gallery"
  images={[
    { src: "/images/projects/capstone/capstone-1.webp", alt: "NAO robot setup" },
    { src: "/images/projects/capstone/capstone-2.webp", alt: "Backpack prototype concept" },
    { src: "/images/projects/capstone/capstone-3.webp", alt: "External processing box concept" },
    { src: "/images/projects/capstone/capstone-4.webp", alt: "Table design concept" },
    { src: "/images/projects/capstone/capstone-5.webp", alt: "Front of table with cameras" },
    { src: "/images/projects/capstone/capstone-6.webp", alt: "Back panel with controls" },
    { src: "/images/projects/capstone/capstone-7.webp", alt: "Bottom with leg mounting" },
    { src: "/images/projects/capstone/capstone-8.webp", alt: "Leg riser mechanism" },
    { src: "/images/projects/capstone/capstone-9.webp", alt: "Corner guards" },
    { src: "/images/projects/capstone/capstone-10.webp", alt: "Software pipeline flowchart" },
    { src: "/images/projects/capstone/capstone-11.webp", alt: "LLM processing setup" },
    { src: "/images/projects/capstone/capstone-12.webp", alt: "Text-to-speech system" },
    { src: "/images/projects/capstone/capstone-15.webp", alt: "Internal electronics layout" },
    { src: "/images/projects/capstone/capstone-20.webp", alt: "Robot performing for live audience" },
    { src: "/images/projects/capstone/capstone-21.webp", alt: "Audience engagement" },
    { src: "/images/projects/capstone/capstone-25.webp", alt: "Future improvements diagram" },
    { src: "/images/projects/capstone/capstone-30.webp", alt: "Hardware upgrade possibilities" },
    { src: "/images/projects/capstone/capstone-46.webp", alt: "Final robot performance" },
  ]}
/>

</section>

<section>
## Final Report

### Abstract

---

As robots become more integrated into our everyday lives, there exists the need for robots to be able to engage naturally with humans. To advance this aim, our team has worked on developing a robotic system capable of performing a comedy routine while interacting with its audience and adjusting its performance based on the reactions of the crowd. We also sought to incorporate a Large Language Model (LLM) to generate responses to audience input in a crowd-work-like manner.

To accomplish these goals, we incorporated cameras, microphones, and an external processing unit to capture and process the audience's reactions. The extra hardware was kept separate from the NAO robot to not interfere with its stability and to keep the whole system portable. A table was constructed to house the extra hardware and provide a stable platform for the robot to perform on. The design of this table was kept discrete and painted black, keeping the audience's attention on the robot itself. A Raspberry Pi 5 was chosen as our external processor so that the system could be powered from a battery to keep the system portable.

The main challenge of this project arose from the limitations of the Raspberry Pi while trying to run an LLM. We largely achieved these goals with some challenges, mainly with the facial reading portion. The industry as a whole has a hard time accurately determining the affect of a person based solely on their facial expressions, so this part of our project was reduced to focus on the LLM implementation. With the current state of LLM research being performed on massive multi-million dollar supercomputers, the ability for this project to provide a convincing performance running on low-end hardware was a great success.

### Background & Introduction

---

<ImageTextBlock src="/images/projects/capstone/capstone-1.webp" alt="NAO robot setup" caption="NAO V6 robot by Aldebaran performing comedy" size="lg" position="right">

Robotic interaction continues to evolve, with comedy emerging as an impactful medium to foster human-robot rapport. Comedy has been a way for people to both entertain and connect closely for as long as we can remember. It is used as a way for others to get stories off their chest in a light-hearted way, as a form of relaxation to melt the stresses of the day away, and as a way to feel a little closer to their friends and audience through sharing a positive memory.

Our project scope focuses on furthering research in the Human-Robot Interaction (HRI) field using a robotic stand-up comedian through adding facial expression reading and the use of LLMs to dynamically change its repertoire and repartee. The device used to perform the stand-up comedy routines is a NAO V6 humanoid robot, created by Aldebaran.

</ImageTextBlock>

The current comedic capabilities of this robot were exclusively based on audio cues, through methods including audio thresholding and machine learning techniques to determine audience responses. Our goal was to increase the robots' existing capabilities by using visual input to determine the audience's facial expressions and to implement a large-language model (LLM) to create new repartee.

By benefiting Professor Fitter's work, we're providing advances to the HRI field by refining a way for robots to connect to others more naturally. With comedy being such an integral part of interhuman interaction, we hope to bring this form of connection to the interaction between humans and robots.

### Design Process

---

The current sensing capabilities of the NAO 6 robot were accomplished using an internal microphone and volume level monitoring. Using the built-in cameras was the next logical evolution; however, the computational power of this robot platform proved limited. Run by a low-power Intel Atom processor released in 2013, it simply was not performant enough. Our team identified that the additional sensing or language abilities we wanted would require an off-board coprocessor.

Dr. Naomi Fitter's requirements for this new device included a fool-proof design, where it would simply boot, rapidly pair, and send data to the NAO 6 robot for use. Speed was also a requirement. As previous research into the cadence of joke delivery presented, timing was key. Thus, the NAO 6 robot would need to know as quickly as possible how the audience reacted or responded to a given line.

<ImageTextBlock src="/images/projects/capstone/capstone-2.webp" alt="Initial backpack prototype concept" caption="First prototype idea: processing backpack for NAO robot" size="lg" position="left">

#### Prototype 1: Backpack Design

Initial ideas for a co-processing unit included a pseudo "backpack" for the NAO 6 robot, which would house additional processing power while being physically tethered to the robot itself. This device would use onboard sensors and would make use of a high-speed network connection over a tether. Power would be provided from the robot—reducing the need to charge multiple devices.

However, the NAO 6 robot proved to be extremely sensitive to changes in weight balance or center of gravity. Additionally, overall power consumption would need to be tested, as the NAO 6 robot only lasts one to two hours operating standalone.

</ImageTextBlock>

<ImageTextBlock src="/images/projects/capstone/capstone-3.webp" alt="External processing box concept" caption="Second prototype: external processing unit" size="lg" position="right">

#### Prototype 2: External Box

Another option included an external processing unit completely separated from the NAO 6 robot, where power consumption limitations would be negated as it would connect to a power outlet. It also would not have space limitations, as it could be placed discretely on or off stage and wirelessly connect to the robot.

However, this box would not be in an ideal location to provide its own sensing capabilities. Microphone audio is sensitive to proximity, and this device would be placed entirely away from a crowd. This design would also decrease the overall portability of the system.

</ImageTextBlock>

<ImageTextBlock src="/images/projects/capstone/capstone-4.webp" alt="Table design concept" caption="Final prototype: integrated table design" size="lg" position="left">

#### Prototype 3: Integrated Table (Final Design)

Our team considered modifying the table (an IKEA LACK side table) the NAO 6 robot was often placed on top of. This design had the benefit of allowing sensors to be placed close to the robot, nearest to the crowd. It also directly replaces the existing table, minimizing the impact of portability to our client's process.

For our intended application, we felt this was the best option for its ease to our client and future upgradability. The downsides could be engineered through in a straightforward manner.

</ImageTextBlock>

### Design Solution - Table Construction

---

We were confident with our design of the system based on all the research we had done in the first term of this project, and we set out to build it. Both aspects of the full system ended up taking longer than we had hoped. The table build ended up taking 6 weeks to complete with small issues arising that needed new parts to be manufactured. The software pipeline took longer as we spent time hunting down bugs, chased speed increases, and searched for better systems to integrate that would provide us with the features and timing we needed to provide a convincing performance.

<ImageTextBlock src="/images/projects/capstone/capstone-5.webp" alt="Front of table with cameras and microphone" caption="Front panel with integrated cameras and microphone" size="lg" position="right">

The table was designed to completely house the cameras, microphone, Raspberry Pi 5, and battery. The front of the table hosts the cameras and a microphone. This placement made sure that the system would remain unobtrusive but still have full view of the audience in order to determine their reactions.

We added vents on the sides of the table so that air would flow, ensuring the Raspberry Pi wouldn't overheat. We also made these vents larger than needed in the event that a future project might want to put a GPU in the table as well. The extra venting would support the substantial cooling required to run a desktop-class GPU.

</ImageTextBlock>

<ImageTextBlock src="/images/projects/capstone/capstone-6.webp" alt="Back panel with controls" caption="Rear panel with power controls and monitoring" size="lg" position="left">

On the back of the table, we added all the ports and switches required to turn the system on and monitor its status. The button panel accommodates the power switch, record switch, USB-C charging port, 2 status LEDs, and the battery screen. A cutout was necessary to allow the user to see the screen built into the battery, allowing them to have visual feedback on the charge percentage. The rear also includes a handle attached in the middle for easy carrying.

</ImageTextBlock>

<ImageTextBlock src="/images/projects/capstone/capstone-7.webp" alt="Bottom of table with leg mounting" caption="Bottom view showing collapsible leg system" size="lg" position="right">

The bottom of the table houses the legs during transportation. The method of using screw-on legs that snap into compliant mounts worked very well. The clips were carefully designed so the whole mount would flex when a leg was inserted without distorting the part and potentially breaking it.

</ImageTextBlock>

<ImageTextBlock src="/images/projects/capstone/capstone-8.webp" alt="Leg riser mechanism" caption="Custom risers for leg mounting system" size="lg" position="left">

Additional problems arose with the wood inserts that the legs would screw into. They were designed to screw into the wood and relied on a hex tool to do so, such that the threads didn't travel to the top of the insert. To solve this problem, we designed risers that would sit between the table and the threaded insert to extend them out from its face. The added benefit was that the legs would seat against the risers and not the table itself, reducing wear on easily replaceable parts.

</ImageTextBlock>

<ImageTextBlock src="/images/projects/capstone/capstone-9.webp" alt="Corner guards" caption="Protective corner guards for transport" size="lg" position="right">

Corner guards were installed along the table to protect it during transport as well as the front camera mount when the table was set down like a briefcase. The corner guards also served to cover up some of the misalignments in the wood sections.

</ImageTextBlock>

### Software Architecture

---

The software pipeline for the Comedy Robot integrates multiple components to analyze audience reactions and generate dynamic comedic responses in real time. At the core of the system is a facial affect recognition module, which utilizes two visible-light cameras embedded in the table to capture audience expressions.

<ImageTextBlock src="/images/projects/capstone/capstone-10.webp" alt="Software pipeline flowchart" caption="Crowd-work pipeline showing data flow" size="lg" position="right">

#### Facial Affect Recognition

These images are processed using DeepFace, a machine learning framework that classifies facial expressions into positive, neutral, or negative categories. By analyzing audience sentiment over a brief time window—approximately two seconds after a joke is delivered—the system determines whether the response was favorable and adjusts the performance accordingly.

Given the limitations of facial affect reading, particularly with the challenges of classifying nuanced emotions, the system achieved an overall accuracy of 61.3%, with minor misclassifications in 26.5% of cases and critical errors in 12.2% of cases.

</ImageTextBlock>

<ImageTextBlock src="/images/projects/capstone/capstone-11.webp" alt="LLM processing setup" caption="Raspberry Pi 5 running Llama 3.2-1b" size="lg" position="left">

#### Crowd-Work Response Pipeline

Beyond passive audience sentiment analysis, the Comedy Robot actively engages with the audience through a crowd-work response pipeline that enables it to generate personalized jokes. The interaction begins with the robot posing a simple question, such as "Where are you from?" or "What's your favorite hobby?"

Audience responses are captured through a microphone and processed using Vosk, an offline speech-to-text (STT) engine. The transcribed text is passed to Llama 3.2-1b, which generates a contextually relevant comedic response.

</ImageTextBlock>

<ImageTextBlock src="/images/projects/capstone/capstone-12.webp" alt="Text-to-speech system" caption="Piper TTS synthesis for natural robot voice" size="lg" position="right">

#### Real-Time Response Optimization

Once the joke is generated, it is passed to Piper TTS, a lightweight text-to-speech synthesis engine, which converts the response into spoken dialogue for the NAO robot.

A crucial challenge was ensuring real-time responsiveness to maintain comedic timing. The system implements pre-loaded language models, batch processing, and a buffer phrase system allowing the robot to output pre-recorded filler lines while waiting for the LLM to generate the joke. These optimizations ensured that the complete pipeline averaged 4.3 seconds from audience input to joke delivery.

</ImageTextBlock>

### Assembly & Integration

---

<ImageTextBlock src="/images/projects/capstone/capstone-15.webp" alt="Internal electronics layout" caption="Internal component layout and wiring" size="lg" position="left">

The system is completely self-contained and enclosed to ensure weather-safe handling during transportation. A DIN rail was installed on the opposite side of the table from the Raspberry Pi to allow for future installation of a desktop GPU if needed, with access to an air vent to facilitate cooling needs.

The currently installed battery is capable of delivering 100W, with roughly 25W required by the Raspberry Pi itself. This provides several hours of runtime for performances without requiring external power.

</ImageTextBlock>

### Testing & Results

---

<ImageTextBlock src="/images/projects/capstone/capstone-20.webp" alt="Robot performing for live audience" caption="Live performance testing with audience" size="lg" position="right">

During the final evaluation, comprehensive testing was conducted both in a controlled laboratory environment and several times during live public performances. The primary objective was to assess the robot's capability to monitor and dynamically adapt its performance based on real-time audience feedback.

Overall, the system performed successfully, effectively interpreting and responding to visual and auditory cues from audience members.

</ImageTextBlock>

<ImageTextBlock src="/images/projects/capstone/capstone-21.webp" alt="Audience engagement" caption="Positive audience reactions during testing" size="lg" position="left">

The table was largely a success and performs the requirements set by our client well. It is a solid enclosure for the electrical equipment and provides a stable platform for the robot to stand on during its performances. It is also highly portable and energy efficient, with the battery lasting for several hours while running a performance.

The leg mechanism works very well and makes collapsing the system easy for transport.

</ImageTextBlock>

### Future Work

---

<ImageTextBlock src="/images/projects/capstone/capstone-25.webp" alt="Future improvements diagram" caption="Areas identified for future enhancement" size="lg" position="right">

#### Facial Affect System Improvements

Further work could be done to improve sensitivity and accuracy. The system is currently only reliable enough to gauge an audience's reaction when there are many visible faces. We propose not taking the absolute emotion of the group, but rather tracking the baseline emotions on an individual level, and calculating when the audience is reacting by measuring when each person's emotion changes relative to their baseline.

</ImageTextBlock>

<ImageTextBlock src="/images/projects/capstone/capstone-30.webp" alt="Hardware upgrade possibilities" caption="Potential for GPU integration" size="lg" position="left">

#### Computational Power Upgrades

There is the possibility of increasing the computational power of the system which would allow for a more responsive and capable LLM system. Either a different central processing board could be installed, or the current Raspberry Pi 5 could be augmented with a desktop GPU. The power system would need to be heavily considered before performing this upgrade.

</ImageTextBlock>

### Conclusion

---

Our team applied robust engineering design principles to develop a responsive robotic system specifically designed for effective human interaction. Our system incorporates audience emotion analysis and responsive communication, significantly enhancing user engagement and satisfaction.

This project underscores the growing significance of Human-Robot Interaction (HRI) within entertainment, hospitality, and general service sectors. As robotic applications continue to expand in these fields, the importance of nuanced, thoughtful, and reliable human interactions will become increasingly central to their effectiveness and adoption.

### Acknowledgements

---

**Professor Naomi Fitter**, for their excellent guidance on our project trajectory, controlling scope creep, and assisting where needed on architecture design and joke writing.

**Graduate Student Aidan Beery**, for their assistance in processing voice data and training of the Piper TTS model used on this device.

</section>
